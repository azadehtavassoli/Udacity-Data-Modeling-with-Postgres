## Data Engineer Project (Udacity Program- JULY 2022): Data Modeling with Postgres

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis and bring you on the project. The aim of the project is to create a database schema and ETL pipeline for this analysis. 

### Datasets Description

The song dataset is a subset of real data from the [Million Song Dataset]. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

The log dataset consists of log files in JSON format generated by [this event simulator]. These simulate activity logs from a music streaming app based on specified configurations.


### The ETL Pipeline Instruction

* Execute the script to generate the database and its tables by executing `python3 create_tables.py`.
* Load the data and insert it into the database by executing `python3 etl.py`.

**Note:** Please remember to run `create_tables.py` for each run to drop and recreate the database and the tables

### Schema Design

* The fact table `songplays` stores the records in log data associated with song plays. 
* The dimension table `users` stores the users in the app.
* The dimension table `song` stores the songs in the music database.
* The dimension table `artists` stores the artists in the music database.
* The dimension table `time` stores the timestamps of records in song plays broken down into specific units.

### The key objective of this database

This database enables to have all songs, artists, users, and song plays combined in a single database. As the result, the company is not forced to store the data in a specific structure which makes it comparatively easier to perform future analysis such as song popularity analysis or extracting geographical-based information and analysis. 

### Files in the Repository
* `create_tables.py`: Python file to drop the database and the tables if they exist and create a new database and the tables.
* `etl.py`: Python file responsible for ETL pipeline.
* `sql.py`: Python file with relevant SQL queries for creating/dropping tables, inserting records as described in Schema Design Section, and selecting query for select from song table.
* `etl.ipynb`: Jupiter Notebook performing the ETL process on only one song file and log files. The functions in this notebook, used in etl.py eventually.
* `test.ipynb`: Jupiter Notebook testing the project and its sanity.
* `readme.md`: a short description of the project, guide on running the project, and details on the repository files.
* `data`: folder including:
  * `log_data` folder with JSON log files partitioned based on year and month,
  * `song+data` folder with JSON song files